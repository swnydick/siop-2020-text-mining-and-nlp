---
title: "Machine Learning to Classify Text"
author: "Korn Ferry Institute"
date: "01/20/2020"
output:
  rmdformats::material:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: true
    highlight: pygments
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TODO -- have to rework this to point to data after it's been cleaned up and scored by J&S

# Advanced NLP

After wrangling our data to be like so...

```{r dependencies}

require(data.table)
require(keras)
require(caret)
require(magrittr)
require(tm)
# Important! First timers run this line: 
#keras::install_keras()

dt <- fread("../data/siop_2020_txt_long.csv")

dt <- dt[text != "" & type != "title", ]

dt[, text := gsub("^.*</b><br/>", "" ,text)] 

dt[, text2 := tm::removePunctuation(text)]
dt[, text2 := tm::removeWords(text2, tm::stopwords("en"))]
dt[, text2 := tolower(text2)]
dt[, target := ifelse(type == "pro", 1, 0)]
# ...etc until text is good and clean...

```

copy pasta for now from:

https://www.kaggle.com/taindow/simple-lstm-with-r


todo: 

* Make own embeddings object 
* optimize topology
* add flair

```{r}

max_words <- 5000 # max number of words to use
max_len   <- 64 # text cutoff at n (make bigger for longer texts at expense of size/time)

# Create layer to reduce text to integer vector
# ...Then in embedding create distance vector betwween tokenized vec

# First, the tokenizer (to create the int vectors)
tokenizer <- text_tokenizer(num_words = max_words) %>%
             fit_text_tokenizer(dt$text2)

# Apply tokenizer
txt_sequences <- texts_to_sequences(tokenizer, dt$text2)
text_data     <- pad_sequences(txt_sequences, maxlen = max_len)
# Inspect to see padding


train_idx <- caret::createDataPartition(dt$type, p = .75, list = FALSE)

x_train <- text_data[train_idx, ]
x_test  <- text_data[-train_idx,]

y_train <- dt[train_idx, target]
y_test  <- dt[-train_idx, target]

mean(y_test)
mean(y_train)
# Very similar balance:) 


# Embeddings give similarity between words used in English
# https://www.kaggle.com/mmanishh/fast-text-word-embeddings/version/1#wiki-news-300d-1M.vec
lines <- readLines('../data/wiki-news-300d-1M/wiki-news-300d-1M.vec')

#wiki_embeddings_env = new.env(hash = TRUE, parent = emptyenv())
wiki_embeddings <- list()

lines <- lines[2:length(lines)]

pb <- txtProgressBar(min = 0, max = length(lines), style = 3)
for (i in 1:length(lines)){
  line   <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word   <- values[[1]]
  wiki_embeddings[[word]] <- as.double(values[-1])
  setTxtProgressBar(pb, i)
}


# Create our embedding matrix
word_index         <- tokenizer$word_index 
wiki_embedding_dim <- 300
wiki_embedding_mx  <- array(0, c(max_words, wiki_embedding_dim))

for (word in names(word_index)){
  index <- word_index[[word]]
  if (index < max_words){
    wiki_embedding_vec <- wiki_embeddings[[word]]
    if (!is.null(wiki_embedding_vec))
      wiki_embedding_mx[index+1,] <- wiki_embedding_vec # Words without an embedding are all zeros
  }
}


# gc()



# Setup input

input <- layer_input(
  shape = list(NULL),
  dtype = "int32",
  name  = "input"
)

# Model layers

embedding <- input %>% 
    layer_embedding(input_dim = max_words, output_dim = wiki_embedding_dim, name = "embedding")

lstm <- embedding %>% 
    layer_lstm(units = 24, dropout = 0.25, recurrent_dropout = 0.25, return_sequences = FALSE, name = "lstm")

dense <- lstm %>%
    layer_dense(units = 64, activation = "tanh", name = "dense") 

dense2 <- dense %>%
    layer_dense(units = 64, activation = "tanh", name = "dense") 

predictions <- dense %>% 
    layer_dense(units = 1, activation = "sigmoid", name = "predictions")

  
# Bring model together

model <- keras_model(input, predictions)

# Freeze the embedding weights initially to prevent updates propgating back through and ruining our embedding

get_layer(model, name = "embedding") %>% 
  set_weights(list(wiki_embedding_mx)) %>% 
  freeze_weights()


# Compile

model %>% compile(
  optimizer = optimizer_adam(),
  loss = "binary_crossentropy",
  metrics = "binary_accuracy"
)


# Print architecture (plot_model isn't implemented in the R package yet)

print(model)


history <- model %>% fit(
  x_train,
  y_train,
  batch_size = 512,
  validation_data = list(x_test, y_test),
  epochs = 32,
  shuffle = TRUE,
  view_metrics = FALSE,
  verbose = 0
)

# Look at training results

print(history)
plot(history)

y_pred <- predict(model, x_test)
y_pred <- factor(round(y_pred))
y_real <- factor(round(y_test))

confusionMatrix(y_pred, y_real)

#saveRDS(model, "scratch/model1.rds")



```



